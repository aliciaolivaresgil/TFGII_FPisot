{"description": "A partir del RAG creado se va a intentar introducir el concepto de agentes para obtener informaci\u00f3n que nutra al modelo.", "icon": null, "is_component": false, "webhook": false, "id": "bb567874-0138-4cd2-84b4-6bd185102636", "user_id": "cac1838a-da27-4f2e-84ee-136035e3ae2f", "name": "Eco-City Tours", "icon_bg_color": null, "updated_at": "2024-08-27T12:21:40+00:00", "endpoint_name": null, "data": {"nodes": [{"id": "ChatInput-NYZeX", "type": "genericNode", "position": {"x": 776, "y": 150.25}, "data": {"type": "ChatInput", "node": {"template": {"_type": "Component", "files": {"trace_as_metadata": true, "file_path": "", "fileTypes": ["txt", "md", "mdx", "csv", "json", "yaml", "yml", "xml", "html", "htm", "pdf", "docx", "py", "sh", "sql", "js", "ts", "tsx", "jpg", "jpeg", "png", "bmp", "image"], "list": true, "required": false, "placeholder": "", "show": true, "name": "files", "value": "", "display_name": "Files", "advanced": true, "dynamic": false, "info": "Files to be sent with the message.", "title_case": false, "type": "file", "_input_type": "FileInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "input_value": {"trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "\u00bf Necesito una reserva ?", "display_name": "Text", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Message to be passed as input.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}, "sender": {"trace_as_metadata": true, "options": ["Machine", "User"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "sender", "value": "User", "display_name": "Sender Type", "advanced": true, "dynamic": false, "info": "Type of sender.", "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "sender_name": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "sender_name", "value": "", "display_name": "Sender Name", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Name of the sender.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "session_id": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "session_id", "value": "", "display_name": "Session ID", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The session ID of the chat. If empty, the current session ID parameter will be used.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "should_store_message": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "should_store_message", "value": true, "display_name": "Store Messages", "advanced": true, "dynamic": false, "info": "Store the message in the history.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}}, "description": "Get chat inputs from the Playground.", "icon": "ChatInput", "base_classes": ["Message"], "display_name": "Chat Input", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "message", "display_name": "Message", "method": "message_response", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value", "should_store_message", "sender", "sender_name", "session_id", "files"], "beta": false, "edited": false, "lf_version": "1.0.16"}, "id": "ChatInput-NYZeX"}, "selected": false, "width": 384, "height": 384, "positionAbsolute": {"x": 776, "y": 150.25}, "dragging": false}, {"id": "TextInput-6XxgF", "type": "genericNode", "position": {"x": 258, "y": 223.25}, "data": {"type": "TextInput", "node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "input_value": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "Fer", "display_name": "Text", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Text to be passed as input.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}}, "description": "Get text inputs from the Playground.", "icon": "type", "base_classes": ["Message"], "display_name": "Name", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "text", "display_name": "Text", "method": "text_response", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value"], "beta": false, "edited": false, "lf_version": "1.0.16"}, "id": "TextInput-6XxgF"}, "selected": false, "width": 384, "height": 298, "dragging": false, "positionAbsolute": {"x": 258, "y": 223.25}}, {"id": "Prompt-4QaZX", "type": "genericNode", "position": {"x": 2236.010808994086, "y": 411.3301935405039}, "data": {"type": "Prompt", "node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "template": {"trace_as_input": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "template", "value": "Hola, puedes contestar al usuario basado en el siguiente contexto:\n\nEste es el historial de mensajes: {history}\n\nLa pregunta del usuario ser\u00e1 esta: \"Tu rol es un gu\u00eda tur\u00edstico comprometido con el medio ambiente preocupado por la gentrificaci\u00f3n de las ciudades y el turismo masivo. \n                   Dime 3 sitios para visitar en {city}\n                   Quiero que la respuesta sea S\u00d3LO la informaci\u00f3n de los sitios en formato \n                   json, sin saludos, ni recomendaciones, s\u00f3lo el json con las siguientes propiedades: nombre, coordenadas gps, breve descripci\u00f3n, una url de una imagen del sitio y una url si existe de informaci\u00f3n del sitio.\"\n\nEl contexto es este: {context}", "display_name": "Template", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "prompt", "_input_type": "PromptInput"}, "context": {"field_type": "str", "required": false, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "", "fileTypes": [], "file_path": "", "password": false, "name": "context", "display_name": "context", "advanced": false, "input_types": ["Message", "Text"], "dynamic": false, "info": "", "load_from_db": false, "title_case": false, "type": "str"}, "history": {"field_type": "str", "required": false, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "", "fileTypes": [], "file_path": "", "password": false, "name": "history", "display_name": "history", "advanced": false, "input_types": ["Message", "Text"], "dynamic": false, "info": "", "load_from_db": false, "title_case": false, "type": "str"}, "city": {"field_type": "str", "required": false, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "", "fileTypes": [], "file_path": "", "password": false, "name": "city", "display_name": "city", "advanced": false, "input_types": ["Message", "Text"], "dynamic": false, "info": "", "load_from_db": false, "title_case": false, "type": "str"}}, "description": "Create a prompt template with dynamic variables.", "icon": "prompts", "is_input": null, "is_output": null, "is_composition": null, "base_classes": ["Message"], "name": "", "display_name": "Prompt", "documentation": "", "custom_fields": {"template": ["history", "city", "context"]}, "output_types": [], "full_path": null, "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "prompt", "hidden": null, "display_name": "Prompt Message", "method": "build_prompt", "value": "__UNDEFINED__", "cache": true}], "field_order": ["template"], "beta": false, "error": null, "edited": false}, "id": "Prompt-4QaZX"}, "selected": false, "width": 384, "height": 584, "dragging": false, "positionAbsolute": {"x": 2236.010808994086, "y": 411.3301935405039}}, {"id": "Memory-RrURK", "type": "genericNode", "position": {"x": 777.7712942488849, "y": 572.5205055647033}, "data": {"type": "Memory", "node": {"template": {"_type": "Component", "memory": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "memory", "value": "", "display_name": "External Memory", "advanced": false, "input_types": ["BaseChatMessageHistory"], "dynamic": false, "info": "Retrieve messages from an external memory. If empty, it will use the Langflow tables.", "title_case": false, "type": "other", "_input_type": "HandleInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langchain.memory import ConversationBufferMemory\n\nfrom langflow.custom import Component\nfrom langflow.field_typing import BaseChatMemory\nfrom langflow.helpers.data import data_to_text\nfrom langflow.inputs import HandleInput\nfrom langflow.io import DropdownInput, IntInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import LCBuiltinChatMemory, get_messages\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER\n\n\nclass MemoryComponent(Component):\n    display_name = \"Chat Memory\"\n    description = \"Retrieves stored chat messages from Langflow tables or an external memory.\"\n    icon = \"message-square-more\"\n    name = \"Memory\"\n\n    inputs = [\n        HandleInput(\n            name=\"memory\",\n            display_name=\"External Memory\",\n            input_types=[\"BaseChatMessageHistory\"],\n            info=\"Retrieve messages from an external memory. If empty, it will use the Langflow tables.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, \"Machine and User\"],\n            value=\"Machine and User\",\n            info=\"Filter by sender type.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Filter by sender name.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"n_messages\",\n            display_name=\"Number of Messages\",\n            value=100,\n            info=\"Number of messages to retrieve.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"order\",\n            display_name=\"Order\",\n            options=[\"Ascending\", \"Descending\"],\n            value=\"Ascending\",\n            info=\"Order of the messages.\",\n            advanced=True,\n        ),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.\",\n            value=\"{sender_name}: {text}\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Messages (Data)\", name=\"messages\", method=\"retrieve_messages\"),\n        Output(display_name=\"Messages (Text)\", name=\"messages_text\", method=\"retrieve_messages_as_text\"),\n        Output(display_name=\"Memory\", name=\"lc_memory\", method=\"build_lc_memory\"),\n    ]\n\n    def retrieve_messages(self) -> Data:\n        sender = self.sender\n        sender_name = self.sender_name\n        session_id = self.session_id\n        n_messages = self.n_messages\n        order = \"DESC\" if self.order == \"Descending\" else \"ASC\"\n\n        if sender == \"Machine and User\":\n            sender = None\n\n        if self.memory:\n            # override session_id\n            self.memory.session_id = session_id\n\n            stored = self.memory.messages\n            # langchain memories are supposed to return messages in ascending order\n            if order == \"DESC\":\n                stored = stored[::-1]\n            if n_messages:\n                stored = stored[:n_messages]\n            stored = [Message.from_lc_message(m) for m in stored]\n            if sender:\n                expected_type = MESSAGE_SENDER_AI if sender == MESSAGE_SENDER_AI else MESSAGE_SENDER_USER\n                stored = [m for m in stored if m.type == expected_type]\n        else:\n            stored = get_messages(\n                sender=sender,\n                sender_name=sender_name,\n                session_id=session_id,\n                limit=n_messages,\n                order=order,\n            )\n        self.status = stored\n        return stored\n\n    def retrieve_messages_as_text(self) -> Message:\n        stored_text = data_to_text(self.template, self.retrieve_messages())\n        self.status = stored_text\n        return Message(text=stored_text)\n\n    def build_lc_memory(self) -> BaseChatMemory:\n        if self.memory:\n            chat_memory = self.memory\n        else:\n            chat_memory = LCBuiltinChatMemory(flow_id=self.flow_id, session_id=self.session_id)\n        return ConversationBufferMemory(chat_memory=chat_memory)\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "n_messages": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "n_messages", "value": 100, "display_name": "Number of Messages", "advanced": true, "dynamic": false, "info": "Number of messages to retrieve.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "order": {"trace_as_metadata": true, "options": ["Ascending", "Descending"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "order", "value": "Ascending", "display_name": "Order", "advanced": true, "dynamic": false, "info": "Order of the messages.", "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "sender": {"trace_as_metadata": true, "options": ["Machine", "User", "Machine and User"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "sender", "value": "Machine and User", "display_name": "Sender Type", "advanced": true, "dynamic": false, "info": "Filter by sender type.", "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "sender_name": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "sender_name", "value": "", "display_name": "Sender Name", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "Filter by sender name.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "session_id": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "session_id", "value": "", "display_name": "Session ID", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The session ID of the chat. If empty, the current session ID parameter will be used.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "template": {"trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "template", "value": "{sender_name}: {text}", "display_name": "Template", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}}, "description": "Retrieves stored chat messages from Langflow tables or an external memory.", "icon": "message-square-more", "base_classes": ["BaseChatMemory", "Data", "Message"], "display_name": "Chat Memory", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Data"], "selected": "Data", "name": "messages", "display_name": "Messages (Data)", "method": "retrieve_messages", "value": "__UNDEFINED__", "cache": true}, {"types": ["Message"], "selected": "Message", "name": "messages_text", "display_name": "Messages (Text)", "method": "retrieve_messages_as_text", "value": "__UNDEFINED__", "cache": true}, {"types": ["BaseChatMemory"], "selected": "BaseChatMemory", "name": "lc_memory", "display_name": "Memory", "method": "build_lc_memory", "value": "__UNDEFINED__", "cache": true}], "field_order": ["memory", "sender", "sender_name", "n_messages", "session_id", "order", "template"], "beta": false, "edited": false, "lf_version": "1.0.16"}, "id": "Memory-RrURK"}, "selected": false, "width": 384, "height": 464, "positionAbsolute": {"x": 777.7712942488849, "y": 572.5205055647033}, "dragging": false}, {"id": "OllamaModel-afIXt", "type": "genericNode", "position": {"x": 2763.0804810099557, "y": 213.99276697047816}, "data": {"type": "OllamaModel", "node": {"template": {"_type": "Component", "base_url": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "base_url", "value": "http://localhost:11434", "display_name": "Base URL", "advanced": false, "dynamic": false, "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from typing import Any\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value + \"/api/tags\")\n\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, url: str) -> list[str]:\n        try:\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n                return model_names\n        except Exception as e:\n            raise ValueError(\"Could not retrieve models. Please, make sure Ollama is running.\") from e\n\n    inputs = LCModelComponent._base_inputs + [\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3.1\",\n            info=\"Refer to https://ollama.com/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\",\n            display_name=\"Format\",\n            info=\"Specify the format of the output (e.g., json).\",\n            advanced=True,\n        ),\n        DictInput(\n            name=\"metadata\",\n            display_name=\"Metadata\",\n            info=\"Metadata to add to the run trace.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"tfs_z\",\n            display_name=\"TFS Z\",\n            info=\"Tail free sampling value. (Default: 1)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"Timeout for the request stream.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Limits token selection to top K. (Default: 40)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"Works together with top-k. (Default: 0.9)\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            info=\"Whether to print out response text.\",\n        ),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"system\",\n            display_name=\"System\",\n            info=\"System to use for generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"Template to use for generating text.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)  # type: ignore\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not initialize Ollama LLM.\") from e\n\n        return output  # type: ignore\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "format": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "format", "value": "", "display_name": "Format", "advanced": true, "dynamic": false, "info": "Specify the format of the output (e.g., json).", "title_case": false, "type": "str", "_input_type": "StrInput"}, "input_value": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Input", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MessageInput"}, "metadata": {"trace_as_input": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "metadata", "value": {}, "display_name": "Metadata", "advanced": true, "dynamic": false, "info": "Metadata to add to the run trace.", "title_case": false, "type": "dict", "_input_type": "DictInput"}, "mirostat": {"trace_as_metadata": true, "options": ["Disabled", "Mirostat", "Mirostat 2.0"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "mirostat", "value": "Disabled", "display_name": "Mirostat", "advanced": true, "dynamic": false, "info": "Enable/disable Mirostat sampling for controlling perplexity.", "real_time_refresh": true, "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "mirostat_eta": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "mirostat_eta", "value": "", "display_name": "Mirostat Eta", "advanced": true, "dynamic": false, "info": "Learning rate for Mirostat algorithm. (Default: 0.1)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "mirostat_tau": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "mirostat_tau", "value": "", "display_name": "Mirostat Tau", "advanced": true, "dynamic": false, "info": "Controls the balance between coherence and diversity of the output. (Default: 5.0)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "model_name": {"trace_as_metadata": true, "options": [], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "model_name", "value": "llama3.1", "display_name": "Model Name", "advanced": true, "dynamic": false, "info": "Refer to https://ollama.com/library for more models.", "refresh_button": true, "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "num_ctx": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "num_ctx", "value": "", "display_name": "Context Window Size", "advanced": true, "dynamic": false, "info": "Size of the context window for generating tokens. (Default: 2048)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "num_gpu": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "num_gpu", "value": "", "display_name": "Number of GPUs", "advanced": true, "dynamic": false, "info": "Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "num_thread": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "num_thread", "value": "", "display_name": "Number of Threads", "advanced": true, "dynamic": false, "info": "Number of threads to use during computation. (Default: detected for optimal performance)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "repeat_last_n": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "repeat_last_n", "value": "", "display_name": "Repeat Last N", "advanced": true, "dynamic": false, "info": "How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "repeat_penalty": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "repeat_penalty", "value": "", "display_name": "Repeat Penalty", "advanced": true, "dynamic": false, "info": "Penalty for repetitions in generated text. (Default: 1.1)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "stop_tokens": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "stop_tokens", "value": "", "display_name": "Stop Tokens", "advanced": true, "dynamic": false, "info": "Comma-separated list of tokens to signal the model to stop generating text.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "stream": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "stream", "value": false, "display_name": "Stream", "advanced": true, "dynamic": false, "info": "Stream the response from the model. Streaming works only in Chat.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "system": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "system", "value": "", "display_name": "System", "advanced": true, "dynamic": false, "info": "System to use for generating text.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "system_message": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "system_message", "value": "", "display_name": "System Message", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "System message to pass to the model.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "tags": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "tags", "value": "", "display_name": "Tags", "advanced": true, "dynamic": false, "info": "Comma-separated list of tags to add to the run trace.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "temperature": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "temperature", "value": 0.2, "display_name": "Temperature", "advanced": false, "dynamic": false, "info": "Controls the creativity of model responses.", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "template": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "template", "value": "", "display_name": "Template", "advanced": true, "dynamic": false, "info": "Template to use for generating text.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "tfs_z": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "tfs_z", "value": "", "display_name": "TFS Z", "advanced": true, "dynamic": false, "info": "Tail free sampling value. (Default: 1)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "timeout": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "timeout", "value": "", "display_name": "Timeout", "advanced": true, "dynamic": false, "info": "Timeout for the request stream.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "top_k": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "top_k", "value": "", "display_name": "Top K", "advanced": true, "dynamic": false, "info": "Limits token selection to top K. (Default: 40)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "top_p": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "top_p", "value": "", "display_name": "Top P", "advanced": true, "dynamic": false, "info": "Works together with top-k. (Default: 0.9)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "verbose": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "verbose", "value": false, "display_name": "Verbose", "advanced": false, "dynamic": false, "info": "Whether to print out response text.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}}, "description": "Generate text using Ollama Local LLMs.", "icon": "Ollama", "base_classes": ["LanguageModel", "Message"], "display_name": "Ollama", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "text_output", "display_name": "Text", "method": "text_response", "value": "__UNDEFINED__", "cache": true}, {"types": ["LanguageModel"], "selected": "LanguageModel", "name": "model_output", "display_name": "Language Model", "method": "build_model", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value", "system_message", "stream", "base_url", "model_name", "temperature", "format", "metadata", "mirostat", "mirostat_eta", "mirostat_tau", "num_ctx", "num_gpu", "num_thread", "repeat_last_n", "repeat_penalty", "tfs_z", "timeout", "top_k", "top_p", "verbose", "tags", "stop_tokens", "system", "template"], "beta": false, "edited": false, "lf_version": "1.0.16"}, "id": "OllamaModel-afIXt"}, "selected": false, "width": 384, "height": 595, "dragging": false, "positionAbsolute": {"x": 2763.0804810099557, "y": 213.99276697047816}}, {"id": "ChatOutput-209Yh", "type": "genericNode", "position": {"x": 3208.021154557246, "y": 536.5218471751716}, "data": {"type": "ChatOutput", "node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_AI\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "data_template": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "data_template", "value": "{text}", "display_name": "Data Template", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "input_value": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Text", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Message to be passed as output.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "sender": {"trace_as_metadata": true, "options": ["Machine", "User"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "sender", "value": "Machine", "display_name": "Sender Type", "advanced": true, "dynamic": false, "info": "Type of sender.", "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "sender_name": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "sender_name", "value": "AI", "display_name": "Sender Name", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "Name of the sender.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "session_id": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "session_id", "value": "", "display_name": "Session ID", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The session ID of the chat. If empty, the current session ID parameter will be used.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "should_store_message": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "should_store_message", "value": true, "display_name": "Store Messages", "advanced": true, "dynamic": false, "info": "Store the message in the history.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}}, "description": "Display a chat message in the Playground.", "icon": "ChatOutput", "base_classes": ["Message"], "display_name": "Chat Output", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "message", "display_name": "Message", "method": "message_response", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value", "should_store_message", "sender", "sender_name", "session_id", "data_template"], "beta": false, "edited": false, "lf_version": "1.0.16"}, "id": "ChatOutput-209Yh"}, "selected": false, "width": 384, "height": 298, "positionAbsolute": {"x": 3208.021154557246, "y": 536.5218471751716}, "dragging": false}, {"id": "AstraDB-vQlPh", "type": "genericNode", "position": {"x": 1307.1589561960136, "y": -461.6466206724775}, "data": {"type": "AstraDB", "node": {"template": {"_type": "Component", "embedding": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "embedding", "value": "", "display_name": "Embedding or Astra Vectorize", "advanced": false, "input_types": ["Embeddings", "dict"], "dynamic": false, "info": "Allows either an embedding model or an Astra Vectorize configuration.", "title_case": false, "type": "other", "_input_type": "HandleInput"}, "ingest_data": {"trace_as_metadata": true, "list": true, "trace_as_input": true, "required": false, "placeholder": "", "show": true, "name": "ingest_data", "value": "", "display_name": "Ingest Data", "advanced": false, "input_types": ["Data"], "dynamic": false, "info": "", "title_case": false, "type": "other", "_input_type": "DataInput"}, "api_endpoint": {"load_from_db": true, "required": true, "placeholder": "", "show": true, "name": "api_endpoint", "value": "astra_api", "display_name": "API Endpoint", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "API endpoint URL for the Astra DB service.", "title_case": false, "password": true, "type": "str", "_input_type": "SecretStrInput"}, "batch_size": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "batch_size", "value": "", "display_name": "Batch Size", "advanced": true, "dynamic": false, "info": "Optional number of data to process in a single batch.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "bulk_delete_concurrency": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "bulk_delete_concurrency", "value": "", "display_name": "Bulk Delete Concurrency", "advanced": true, "dynamic": false, "info": "Optional concurrency level for bulk delete operations.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "bulk_insert_batch_concurrency": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "bulk_insert_batch_concurrency", "value": "", "display_name": "Bulk Insert Batch Concurrency", "advanced": true, "dynamic": false, "info": "Optional concurrency level for bulk insert operations.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "bulk_insert_overwrite_concurrency": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "bulk_insert_overwrite_concurrency", "value": "", "display_name": "Bulk Insert Overwrite Concurrency", "advanced": true, "dynamic": false, "info": "Optional concurrency level for bulk insert operations that overwrite existing data.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from loguru import logger\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers import docs_to_data\nfrom langflow.inputs import DictInput, FloatInput\nfrom langflow.io import (\n    BoolInput,\n    DataInput,\n    DropdownInput,\n    HandleInput,\n    IntInput,\n    MultilineInput,\n    SecretStrInput,\n    StrInput,\n)\nfrom langflow.schema import Data\n\n\nclass AstraVectorStoreComponent(LCVectorStoreComponent):\n    display_name: str = \"Astra DB\"\n    description: str = \"Implementation of Vector Store using Astra DB with search capabilities\"\n    documentation: str = \"https://python.langchain.com/docs/integrations/vectorstores/astradb\"\n    name = \"AstraDB\"\n    icon: str = \"AstraDB\"\n\n    inputs = [\n        StrInput(\n            name=\"collection_name\",\n            display_name=\"Collection Name\",\n            info=\"The name of the collection within Astra DB where the vectors will be stored.\",\n            required=True,\n        ),\n        SecretStrInput(\n            name=\"token\",\n            display_name=\"Astra DB Application Token\",\n            info=\"Authentication token for accessing Astra DB.\",\n            value=\"ASTRA_DB_APPLICATION_TOKEN\",\n            required=True,\n        ),\n        SecretStrInput(\n            name=\"api_endpoint\",\n            display_name=\"API Endpoint\",\n            info=\"API endpoint URL for the Astra DB service.\",\n            value=\"ASTRA_DB_API_ENDPOINT\",\n            required=True,\n        ),\n        MultilineInput(\n            name=\"search_input\",\n            display_name=\"Search Input\",\n        ),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingest Data\",\n            is_list=True,\n        ),\n        StrInput(\n            name=\"namespace\",\n            display_name=\"Namespace\",\n            info=\"Optional namespace within Astra DB to use for the collection.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"metric\",\n            display_name=\"Metric\",\n            info=\"Optional distance metric for vector comparisons in the vector store.\",\n            options=[\"cosine\", \"dot_product\", \"euclidean\"],\n            advanced=True,\n        ),\n        IntInput(\n            name=\"batch_size\",\n            display_name=\"Batch Size\",\n            info=\"Optional number of data to process in a single batch.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"bulk_insert_batch_concurrency\",\n            display_name=\"Bulk Insert Batch Concurrency\",\n            info=\"Optional concurrency level for bulk insert operations.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"bulk_insert_overwrite_concurrency\",\n            display_name=\"Bulk Insert Overwrite Concurrency\",\n            info=\"Optional concurrency level for bulk insert operations that overwrite existing data.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"bulk_delete_concurrency\",\n            display_name=\"Bulk Delete Concurrency\",\n            info=\"Optional concurrency level for bulk delete operations.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"setup_mode\",\n            display_name=\"Setup Mode\",\n            info=\"Configuration mode for setting up the vector store, with options like 'Sync', 'Async', or 'Off'.\",\n            options=[\"Sync\", \"Async\", \"Off\"],\n            advanced=True,\n            value=\"Sync\",\n        ),\n        BoolInput(\n            name=\"pre_delete_collection\",\n            display_name=\"Pre Delete Collection\",\n            info=\"Boolean flag to determine whether to delete the collection before creating a new one.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"metadata_indexing_include\",\n            display_name=\"Metadata Indexing Include\",\n            info=\"Optional list of metadata fields to include in the indexing.\",\n            advanced=True,\n        ),\n        HandleInput(\n            name=\"embedding\",\n            display_name=\"Embedding or Astra Vectorize\",\n            input_types=[\"Embeddings\", \"dict\"],\n            info=\"Allows either an embedding model or an Astra Vectorize configuration.\",  # TODO: This should be optional, but need to refactor langchain-astradb first.\n        ),\n        StrInput(\n            name=\"metadata_indexing_exclude\",\n            display_name=\"Metadata Indexing Exclude\",\n            info=\"Optional list of metadata fields to exclude from the indexing.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"collection_indexing_policy\",\n            display_name=\"Collection Indexing Policy\",\n            info=\"Optional dictionary defining the indexing policy for the collection.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            advanced=True,\n            value=4,\n        ),\n        DropdownInput(\n            name=\"search_type\",\n            display_name=\"Search Type\",\n            info=\"Search type to use\",\n            options=[\"Similarity\", \"Similarity with score threshold\", \"MMR (Max Marginal Relevance)\"],\n            value=\"Similarity\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"search_score_threshold\",\n            display_name=\"Search Score Threshold\",\n            info=\"Minimum similarity score threshold for search results. (when using 'Similarity with score threshold')\",\n            value=0,\n            advanced=True,\n        ),\n        DictInput(\n            name=\"search_filter\",\n            display_name=\"Search Metadata Filter\",\n            info=\"Optional dictionary of filters to apply to the search query.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n\n    @check_cached_vector_store\n    def build_vector_store(self):\n        try:\n            from langchain_astradb import AstraDBVectorStore\n            from langchain_astradb.utils.astradb import SetupMode\n        except ImportError:\n            raise ImportError(\n                \"Could not import langchain Astra DB integration package. \"\n                \"Please install it with `pip install langchain-astradb`.\"\n            )\n\n        try:\n            if not self.setup_mode:\n                self.setup_mode = self._inputs[\"setup_mode\"].options[0]\n\n            setup_mode_value = SetupMode[self.setup_mode.upper()]\n        except KeyError:\n            raise ValueError(f\"Invalid setup mode: {self.setup_mode}\")\n\n        if not isinstance(self.embedding, dict):\n            embedding_dict = {\"embedding\": self.embedding}\n        else:\n            from astrapy.info import CollectionVectorServiceOptions\n\n            dict_options = self.embedding.get(\"collection_vector_service_options\", {})\n            dict_options[\"authentication\"] = {\n                k: v for k, v in dict_options.get(\"authentication\", {}).items() if k and v\n            }\n            dict_options[\"parameters\"] = {k: v for k, v in dict_options.get(\"parameters\", {}).items() if k and v}\n            embedding_dict = {\n                \"collection_vector_service_options\": CollectionVectorServiceOptions.from_dict(dict_options)\n            }\n            collection_embedding_api_key = self.embedding.get(\"collection_embedding_api_key\")\n            if collection_embedding_api_key:\n                embedding_dict[\"collection_embedding_api_key\"] = collection_embedding_api_key\n\n        vector_store_kwargs = {\n            **embedding_dict,\n            \"collection_name\": self.collection_name,\n            \"token\": self.token,\n            \"api_endpoint\": self.api_endpoint,\n            \"namespace\": self.namespace or None,\n            \"metric\": self.metric or None,\n            \"batch_size\": self.batch_size or None,\n            \"bulk_insert_batch_concurrency\": self.bulk_insert_batch_concurrency or None,\n            \"bulk_insert_overwrite_concurrency\": self.bulk_insert_overwrite_concurrency or None,\n            \"bulk_delete_concurrency\": self.bulk_delete_concurrency or None,\n            \"setup_mode\": setup_mode_value,\n            \"pre_delete_collection\": self.pre_delete_collection or False,\n        }\n\n        if self.metadata_indexing_include:\n            vector_store_kwargs[\"metadata_indexing_include\"] = self.metadata_indexing_include\n        elif self.metadata_indexing_exclude:\n            vector_store_kwargs[\"metadata_indexing_exclude\"] = self.metadata_indexing_exclude\n        elif self.collection_indexing_policy:\n            vector_store_kwargs[\"collection_indexing_policy\"] = self.collection_indexing_policy\n\n        try:\n            vector_store = AstraDBVectorStore(**vector_store_kwargs)\n        except Exception as e:\n            raise ValueError(f\"Error initializing AstraDBVectorStore: {str(e)}\") from e\n\n        self._add_documents_to_vector_store(vector_store)\n        return vector_store\n\n    def _add_documents_to_vector_store(self, vector_store):\n        documents = []\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                raise ValueError(\"Vector Store Inputs must be Data objects.\")\n\n        if documents:\n            logger.debug(f\"Adding {len(documents)} documents to the Vector Store.\")\n            try:\n                vector_store.add_documents(documents)\n            except Exception as e:\n                raise ValueError(f\"Error adding documents to AstraDBVectorStore: {str(e)}\") from e\n        else:\n            logger.debug(\"No documents to add to the Vector Store.\")\n\n    def _map_search_type(self):\n        if self.search_type == \"Similarity with score threshold\":\n            return \"similarity_score_threshold\"\n        elif self.search_type == \"MMR (Max Marginal Relevance)\":\n            return \"mmr\"\n        else:\n            return \"similarity\"\n\n    def _build_search_args(self):\n        args = {\n            \"k\": self.number_of_results,\n            \"score_threshold\": self.search_score_threshold,\n        }\n\n        if self.search_filter:\n            clean_filter = {k: v for k, v in self.search_filter.items() if k and v}\n            if len(clean_filter) > 0:\n                args[\"filter\"] = clean_filter\n        return args\n\n    def search_documents(self) -> list[Data]:\n        vector_store = self.build_vector_store()\n\n        logger.debug(f\"Search input: {self.search_input}\")\n        logger.debug(f\"Search type: {self.search_type}\")\n        logger.debug(f\"Number of results: {self.number_of_results}\")\n\n        if self.search_input and isinstance(self.search_input, str) and self.search_input.strip():\n            try:\n                search_type = self._map_search_type()\n                search_args = self._build_search_args()\n\n                docs = vector_store.search(query=self.search_input, search_type=search_type, **search_args)\n            except Exception as e:\n                raise ValueError(f\"Error performing search in AstraDBVectorStore: {str(e)}\") from e\n\n            logger.debug(f\"Retrieved documents: {len(docs)}\")\n\n            data = docs_to_data(docs)\n            logger.debug(f\"Converted documents to data: {len(data)}\")\n            self.status = data\n            return data\n        else:\n            logger.debug(\"No search input provided. Skipping search.\")\n            return []\n\n    def get_retriever_kwargs(self):\n        search_args = self._build_search_args()\n        return {\n            \"search_type\": self._map_search_type(),\n            \"search_kwargs\": search_args,\n        }\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "collection_indexing_policy": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "collection_indexing_policy", "value": "", "display_name": "Collection Indexing Policy", "advanced": true, "dynamic": false, "info": "Optional dictionary defining the indexing policy for the collection.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "collection_name": {"trace_as_metadata": true, "load_from_db": true, "list": false, "required": true, "placeholder": "", "show": true, "name": "collection_name", "value": "collection", "display_name": "Collection Name", "advanced": false, "dynamic": false, "info": "The name of the collection within Astra DB where the vectors will be stored.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "metadata_indexing_exclude": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "metadata_indexing_exclude", "value": "", "display_name": "Metadata Indexing Exclude", "advanced": true, "dynamic": false, "info": "Optional list of metadata fields to exclude from the indexing.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "metadata_indexing_include": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "metadata_indexing_include", "value": "", "display_name": "Metadata Indexing Include", "advanced": true, "dynamic": false, "info": "Optional list of metadata fields to include in the indexing.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "metric": {"trace_as_metadata": true, "options": ["cosine", "dot_product", "euclidean"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "metric", "value": "", "display_name": "Metric", "advanced": true, "dynamic": false, "info": "Optional distance metric for vector comparisons in the vector store.", "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "namespace": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "namespace", "value": "", "display_name": "Namespace", "advanced": true, "dynamic": false, "info": "Optional namespace within Astra DB to use for the collection.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "number_of_results": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "number_of_results", "value": 4, "display_name": "Number of Results", "advanced": true, "dynamic": false, "info": "Number of results to return.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "pre_delete_collection": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "pre_delete_collection", "value": false, "display_name": "Pre Delete Collection", "advanced": true, "dynamic": false, "info": "Boolean flag to determine whether to delete the collection before creating a new one.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "search_filter": {"trace_as_input": true, "list": true, "required": false, "placeholder": "", "show": true, "name": "search_filter", "value": {}, "display_name": "Search Metadata Filter", "advanced": true, "dynamic": false, "info": "Optional dictionary of filters to apply to the search query.", "title_case": false, "type": "dict", "_input_type": "DictInput"}, "search_input": {"trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "search_input", "value": "", "display_name": "Search Input", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MultilineInput"}, "search_score_threshold": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "search_score_threshold", "value": 0, "display_name": "Search Score Threshold", "advanced": true, "dynamic": false, "info": "Minimum similarity score threshold for search results. (when using 'Similarity with score threshold')", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "search_type": {"trace_as_metadata": true, "options": ["Similarity", "Similarity with score threshold", "MMR (Max Marginal Relevance)"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "search_type", "value": "Similarity", "display_name": "Search Type", "advanced": true, "dynamic": false, "info": "Search type to use", "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "setup_mode": {"trace_as_metadata": true, "options": ["Sync", "Async", "Off"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "setup_mode", "value": "Sync", "display_name": "Setup Mode", "advanced": true, "dynamic": false, "info": "Configuration mode for setting up the vector store, with options like 'Sync', 'Async', or 'Off'.", "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "token": {"load_from_db": true, "required": true, "placeholder": "", "show": true, "name": "token", "value": "astra_token", "display_name": "Astra DB Application Token", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Authentication token for accessing Astra DB.", "title_case": false, "password": true, "type": "str", "_input_type": "SecretStrInput"}}, "description": "Implementation of Vector Store using Astra DB with search capabilities", "icon": "AstraDB", "base_classes": ["Data", "Retriever", "VectorStore"], "display_name": "Astra DB", "documentation": "https://python.langchain.com/docs/integrations/vectorstores/astradb", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Retriever"], "selected": "Retriever", "name": "base_retriever", "display_name": "Retriever", "method": "build_base_retriever", "value": "__UNDEFINED__", "cache": true}, {"types": ["Data"], "selected": "Data", "name": "search_results", "display_name": "Search Results", "method": "search_documents", "value": "__UNDEFINED__", "cache": true}, {"types": ["VectorStore"], "selected": "VectorStore", "name": "vector_store", "display_name": "Vector Store", "method": "cast_vector_store", "value": "__UNDEFINED__", "cache": true}], "field_order": ["collection_name", "token", "api_endpoint", "search_input", "ingest_data", "namespace", "metric", "batch_size", "bulk_insert_batch_concurrency", "bulk_insert_overwrite_concurrency", "bulk_delete_concurrency", "setup_mode", "pre_delete_collection", "metadata_indexing_include", "embedding", "metadata_indexing_exclude", "collection_indexing_policy", "number_of_results", "search_type", "search_score_threshold", "search_filter"], "beta": false, "edited": false, "lf_version": "1.0.16"}, "id": "AstraDB-vQlPh"}, "selected": false, "width": 384, "height": 770, "dragging": false, "positionAbsolute": {"x": 1307.1589561960136, "y": -461.6466206724775}}, {"id": "OllamaEmbeddings-1EssT", "type": "genericNode", "position": {"x": 252.6534970407224, "y": -223.7406206983051}, "data": {"type": "OllamaEmbeddings", "node": {"template": {"_type": "Component", "base_url": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "base_url", "value": "http://localhost:11434", "display_name": "Ollama Base URL", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langchain_community.embeddings import OllamaEmbeddings\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import FloatInput, MessageTextInput, Output\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"model\",\n            display_name=\"Ollama Model\",\n            value=\"llama3.1\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"http://localhost:11434\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n            )  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not connect to Ollama API.\") from e\n        return output\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "model": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "model", "value": "llama3.1", "display_name": "Ollama Model", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "temperature": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "temperature", "value": 0.1, "display_name": "Model Temperature", "advanced": true, "dynamic": false, "info": "", "title_case": false, "type": "float", "_input_type": "FloatInput"}}, "description": "Generate embeddings using Ollama models.", "icon": "Ollama", "base_classes": ["Embeddings"], "display_name": "Ollama Embeddings", "documentation": "https://python.langchain.com/docs/integrations/text_embedding/ollama", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Embeddings"], "selected": "Embeddings", "name": "embeddings", "display_name": "Embeddings", "method": "build_embeddings", "value": "__UNDEFINED__", "cache": true}], "field_order": ["model", "base_url", "temperature"], "beta": false, "edited": false, "lf_version": "1.0.16"}, "id": "OllamaEmbeddings-1EssT"}, "selected": false, "width": 384, "height": 392, "positionAbsolute": {"x": 252.6534970407224, "y": -223.7406206983051}, "dragging": false}, {"id": "SplitText-84vRX", "type": "genericNode", "position": {"x": 749.7925625128828, "y": -497.91828882682944}, "data": {"type": "SplitText", "node": {"template": {"_type": "Component", "data_inputs": {"trace_as_metadata": true, "list": true, "required": false, "placeholder": "", "show": true, "name": "data_inputs", "value": "", "display_name": "Data Inputs", "advanced": false, "input_types": ["Data"], "dynamic": false, "info": "The data to split.", "title_case": false, "type": "other", "_input_type": "HandleInput"}, "chunk_overlap": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "chunk_overlap", "value": 200, "display_name": "Chunk Overlap", "advanced": false, "dynamic": false, "info": "Number of characters to overlap between chunks.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "chunk_size": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "chunk_size", "value": 1000, "display_name": "Chunk Size", "advanced": false, "dynamic": false, "info": "The maximum number of characters in each chunk.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from typing import List\n\nfrom langchain_text_splitters import CharacterTextSplitter\n\nfrom langflow.custom import Component\nfrom langflow.io import HandleInput, IntInput, MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.utils.util import unescape_string\n\n\nclass SplitTextComponent(Component):\n    display_name: str = \"Split Text\"\n    description: str = \"Split text into chunks based on specified criteria.\"\n    icon = \"scissors-line-dashed\"\n    name = \"SplitText\"\n\n    inputs = [\n        HandleInput(\n            name=\"data_inputs\",\n            display_name=\"Data Inputs\",\n            info=\"The data to split.\",\n            input_types=[\"Data\"],\n            is_list=True,\n        ),\n        IntInput(\n            name=\"chunk_overlap\",\n            display_name=\"Chunk Overlap\",\n            info=\"Number of characters to overlap between chunks.\",\n            value=200,\n        ),\n        IntInput(\n            name=\"chunk_size\",\n            display_name=\"Chunk Size\",\n            info=\"The maximum number of characters in each chunk.\",\n            value=1000,\n        ),\n        MessageTextInput(\n            name=\"separator\",\n            display_name=\"Separator\",\n            info=\"The character to split on. Defaults to newline.\",\n            value=\"\\n\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Chunks\", name=\"chunks\", method=\"split_text\"),\n    ]\n\n    def _docs_to_data(self, docs):\n        data = []\n        for doc in docs:\n            data.append(Data(text=doc.page_content, data=doc.metadata))\n        return data\n\n    def split_text(self) -> List[Data]:\n        separator = unescape_string(self.separator)\n\n        documents = []\n        for _input in self.data_inputs:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n\n        splitter = CharacterTextSplitter(\n            chunk_overlap=self.chunk_overlap,\n            chunk_size=self.chunk_size,\n            separator=separator,\n        )\n        docs = splitter.split_documents(documents)\n        data = self._docs_to_data(docs)\n        self.status = data\n        return data\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "separator": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "separator", "value": "\n", "display_name": "Separator", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The character to split on. Defaults to newline.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}}, "description": "Split text into chunks based on specified criteria.", "icon": "scissors-line-dashed", "base_classes": ["Data"], "display_name": "Split Text", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Data"], "selected": "Data", "name": "chunks", "display_name": "Chunks", "method": "split_text", "value": "__UNDEFINED__", "cache": true}], "field_order": ["data_inputs", "chunk_overlap", "chunk_size", "separator"], "beta": false, "edited": false, "lf_version": "1.0.16"}, "id": "SplitText-84vRX"}, "selected": false, "width": 384, "height": 518, "positionAbsolute": {"x": 749.7925625128828, "y": -497.91828882682944}, "dragging": false}, {"id": "ParseData-uD7V0", "type": "genericNode", "position": {"x": 1771.5457347589552, "y": 385.8027792442655}, "data": {"type": "ParseData", "node": {"template": {"_type": "Component", "data": {"trace_as_metadata": true, "list": false, "trace_as_input": true, "required": false, "placeholder": "", "show": true, "name": "data", "value": "", "display_name": "Data", "advanced": false, "input_types": ["Data"], "dynamic": false, "info": "The data to convert to text.", "title_case": false, "type": "other", "_input_type": "DataInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "sep": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "sep", "value": "\n", "display_name": "Separator", "advanced": true, "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "StrInput"}, "template": {"trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "template", "value": "{text}", "display_name": "Template", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}}, "description": "Convert Data into plain text following a specified template.", "icon": "braces", "base_classes": ["Message"], "display_name": "Parse Data", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "text", "display_name": "Text", "method": "parse_data", "value": "__UNDEFINED__", "cache": true}], "field_order": ["data", "template", "sep"], "beta": false, "edited": false, "lf_version": "1.0.16"}, "id": "ParseData-uD7V0"}, "selected": false, "width": 384, "height": 374, "positionAbsolute": {"x": 1771.5457347589552, "y": 385.8027792442655}, "dragging": false}, {"id": "ToolCallingAgent-jZw7Z", "type": "genericNode", "position": {"x": 2182.3904464665334, "y": -738.7322148023276}, "data": {"type": "ToolCallingAgent", "node": {"template": {"_type": "Component", "chat_history": {"trace_as_metadata": true, "list": true, "trace_as_input": true, "required": false, "placeholder": "", "show": true, "name": "chat_history", "value": "", "display_name": "Chat History", "advanced": true, "input_types": ["Data"], "dynamic": false, "info": "", "title_case": false, "type": "other", "_input_type": "DataInput"}, "llm": {"trace_as_metadata": true, "list": false, "required": true, "placeholder": "", "show": true, "name": "llm", "value": "", "display_name": "Language Model", "advanced": false, "input_types": ["LanguageModel"], "dynamic": false, "info": "", "title_case": false, "type": "other", "_input_type": "HandleInput"}, "tools": {"trace_as_metadata": true, "list": true, "required": false, "placeholder": "", "show": true, "name": "tools", "value": "", "display_name": "Tools", "advanced": false, "input_types": ["Tool", "BaseTool"], "dynamic": false, "info": "", "title_case": false, "type": "other", "_input_type": "HandleInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from typing import Optional, List\n\nfrom langchain.agents import create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, PromptTemplate, HumanMessagePromptTemplate\nfrom langflow.base.agents.agent import LCToolsAgentComponent\nfrom langflow.inputs import MultilineInput\nfrom langflow.inputs.inputs import HandleInput, DataInput\nfrom langflow.schema import Data\n\n\nclass ToolCallingAgentComponent(LCToolsAgentComponent):\n    display_name: str = \"Tool Calling Agent\"\n    description: str = \"Agent that uses tools\"\n    icon = \"LangChain\"\n    beta = True\n    name = \"ToolCallingAgent\"\n\n    inputs = LCToolsAgentComponent._base_inputs + [\n        HandleInput(name=\"llm\", display_name=\"Language Model\", input_types=[\"LanguageModel\"], required=True),\n        MultilineInput(\n            name=\"system_prompt\",\n            display_name=\"System Prompt\",\n            info=\"System prompt for the agent.\",\n            value=\"You are a helpful assistant\",\n        ),\n        MultilineInput(\n            name=\"user_prompt\", display_name=\"Prompt\", info=\"This prompt must contain 'input' key.\", value=\"{input}\"\n        ),\n        DataInput(name=\"chat_history\", display_name=\"Chat History\", is_list=True, advanced=True),\n    ]\n\n    def get_chat_history_data(self) -> Optional[List[Data]]:\n        return self.chat_history\n\n    def create_agent_runnable(self):\n        if \"input\" not in self.user_prompt:\n            raise ValueError(\"Prompt must contain 'input' key.\")\n        messages = [\n            (\"system\", self.system_prompt),\n            (\"placeholder\", \"{chat_history}\"),\n            HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[\"input\"], template=self.user_prompt)),\n            (\"placeholder\", \"{agent_scratchpad}\"),\n        ]\n        prompt = ChatPromptTemplate.from_messages(messages)\n        return create_tool_calling_agent(self.llm, self.tools, prompt)\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "handle_parsing_errors": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "handle_parsing_errors", "value": true, "display_name": "Handle Parse Errors", "advanced": true, "dynamic": false, "info": "", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "input_value": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Input", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "max_iterations": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "max_iterations", "value": 15, "display_name": "Max Iterations", "advanced": true, "dynamic": false, "info": "", "title_case": false, "type": "int", "_input_type": "IntInput"}, "system_prompt": {"trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "system_prompt", "value": "You are a helpful assistant", "display_name": "System Prompt", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "System prompt for the agent.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}, "user_prompt": {"trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "user_prompt", "value": "{input}", "display_name": "Prompt", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "This prompt must contain 'input' key.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}, "verbose": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "verbose", "value": true, "display_name": "Verbose", "advanced": true, "dynamic": false, "info": "", "title_case": false, "type": "bool", "_input_type": "BoolInput"}}, "description": "Agent that uses tools", "icon": "LangChain", "base_classes": ["AgentExecutor", "Message"], "display_name": "Tool Calling Agent", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["AgentExecutor"], "selected": "AgentExecutor", "name": "agent", "display_name": "Agent", "method": "build_agent", "value": "__UNDEFINED__", "cache": true}, {"types": ["Message"], "selected": "Message", "name": "response", "display_name": "Response", "method": "message_response", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value", "handle_parsing_errors", "verbose", "max_iterations", "tools", "llm", "system_prompt", "user_prompt", "chat_history"], "beta": true, "edited": false}, "id": "ToolCallingAgent-jZw7Z"}, "selected": true, "width": 384, "height": 611, "positionAbsolute": {"x": 2182.3904464665334, "y": -738.7322148023276}, "dragging": false}, {"id": "WikipediaAPI-ozKXd", "type": "genericNode", "position": {"x": 253.4255550336428, "y": -791.066412663714}, "data": {"type": "WikipediaAPI", "node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from typing import cast\nfrom langchain_community.tools import WikipediaQueryRun\nfrom langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import BoolInput, IntInput, MessageTextInput, MultilineInput\nfrom langflow.schema import Data\n\n\nclass WikipediaAPIComponent(LCToolComponent):\n    display_name = \"Wikipedia API\"\n    description = \"Call Wikipedia API.\"\n    name = \"WikipediaAPI\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Input\",\n        ),\n        MessageTextInput(name=\"lang\", display_name=\"Language\", value=\"en\"),\n        IntInput(name=\"k\", display_name=\"Number of results\", value=4, required=True),\n        BoolInput(name=\"load_all_available_meta\", display_name=\"Load all available meta\", value=False, advanced=True),\n        IntInput(\n            name=\"doc_content_chars_max\", display_name=\"Document content characters max\", value=4000, advanced=True\n        ),\n    ]\n\n    def run_model(self) -> list[Data]:\n        wrapper = self._build_wrapper()\n        docs = wrapper.load(self.input_value)\n        data = [Data.from_document(doc) for doc in docs]\n        self.status = data\n        return data\n\n    def build_tool(self) -> Tool:\n        wrapper = self._build_wrapper()\n        return cast(Tool, WikipediaQueryRun(api_wrapper=wrapper))\n\n    def _build_wrapper(self) -> WikipediaAPIWrapper:\n        return WikipediaAPIWrapper(  # type: ignore\n            top_k_results=self.k,\n            lang=self.lang,\n            load_all_available_meta=self.load_all_available_meta,\n            doc_content_chars_max=self.doc_content_chars_max,\n        )\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "doc_content_chars_max": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "doc_content_chars_max", "value": 4000, "display_name": "Document content characters max", "advanced": true, "dynamic": false, "info": "", "title_case": false, "type": "int", "_input_type": "IntInput"}, "input_value": {"trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Input", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MultilineInput"}, "k": {"trace_as_metadata": true, "list": false, "required": true, "placeholder": "", "show": true, "name": "k", "value": 4, "display_name": "Number of results", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "int", "_input_type": "IntInput"}, "lang": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "lang", "value": "es", "display_name": "Language", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "load_all_available_meta": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "load_all_available_meta", "value": false, "display_name": "Load all available meta", "advanced": true, "dynamic": false, "info": "", "title_case": false, "type": "bool", "_input_type": "BoolInput"}}, "description": "Call Wikipedia API.", "base_classes": ["Data", "Tool"], "display_name": "Wikipedia API", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Data"], "selected": "Data", "name": "api_run_model", "display_name": "Data", "method": "run_model", "value": "__UNDEFINED__", "cache": true}, {"types": ["Tool"], "selected": "Tool", "name": "api_build_tool", "display_name": "Tool", "method": "build_tool", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value", "lang", "k", "load_all_available_meta", "doc_content_chars_max"], "beta": false, "edited": false}, "id": "WikipediaAPI-ozKXd"}, "selected": false, "width": 384, "height": 515, "positionAbsolute": {"x": 253.4255550336428, "y": -791.066412663714}, "dragging": false}, {"id": "TextInput-wm6Jy", "type": "genericNode", "position": {"x": -198.89769237029975, "y": -1285.182912162106}, "data": {"type": "TextInput", "node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "input_value": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "Salamanca, Espa\u00f1a", "display_name": "Text", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Text to be passed as input.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}}, "description": "Get text inputs from the Playground.", "icon": "type", "base_classes": ["Message"], "display_name": "City", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "text", "display_name": "Text", "method": "text_response", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value"], "beta": false, "edited": false}, "id": "TextInput-wm6Jy"}, "selected": false, "width": 384, "height": 298, "positionAbsolute": {"x": -198.89769237029975, "y": -1285.182912162106}, "dragging": false}, {"id": "FirecrawlCrawlApi-1v3BC", "type": "genericNode", "position": {"x": 1758.1735660369038, "y": -748.8699227328705}, "data": {"type": "FirecrawlCrawlApi", "node": {"template": {"_type": "CustomComponent", "crawlerOptions": {"type": "Data", "required": false, "placeholder": "", "list": false, "show": true, "multiline": false, "fileTypes": [], "file_path": "", "password": false, "name": "crawlerOptions", "display_name": "Crawler Options", "advanced": false, "dynamic": false, "info": "Options for the crawler behavior.", "load_from_db": false, "title_case": false}, "pageOptions": {"type": "Data", "required": false, "placeholder": "", "list": false, "show": true, "multiline": false, "fileTypes": [], "file_path": "", "password": false, "name": "pageOptions", "display_name": "Page Options", "advanced": false, "dynamic": false, "info": "The page options to send with the request.", "load_from_db": false, "title_case": false}, "api_key": {"type": "str", "required": true, "placeholder": "", "list": false, "show": true, "multiline": false, "fileTypes": [], "file_path": "", "password": true, "name": "api_key", "display_name": "API Key", "advanced": false, "dynamic": false, "info": "The API key to use Firecrawl API.", "load_from_db": false, "title_case": false, "input_types": ["Text"], "value": null}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "import uuid\nfrom typing import Optional\n\nfrom langflow.custom import CustomComponent\nfrom langflow.schema import Data\n\n\nclass FirecrawlCrawlApi(CustomComponent):\n    display_name: str = \"FirecrawlCrawlApi\"\n    description: str = \"Firecrawl Crawl API.\"\n    name = \"FirecrawlCrawlApi\"\n\n    output_types: list[str] = [\"Document\"]\n    documentation: str = \"https://docs.firecrawl.dev/api-reference/endpoint/crawl\"\n    field_config = {\n        \"api_key\": {\n            \"display_name\": \"API Key\",\n            \"field_type\": \"str\",\n            \"required\": True,\n            \"password\": True,\n            \"info\": \"The API key to use Firecrawl API.\",\n        },\n        \"url\": {\n            \"display_name\": \"URL\",\n            \"field_type\": \"str\",\n            \"required\": True,\n            \"info\": \"The base URL to start crawling from.\",\n        },\n        \"timeout\": {\n            \"display_name\": \"Timeout\",\n            \"field_type\": \"int\",\n            \"info\": \"The timeout in milliseconds.\",\n        },\n        \"crawlerOptions\": {\n            \"display_name\": \"Crawler Options\",\n            \"info\": \"Options for the crawler behavior.\",\n        },\n        \"pageOptions\": {\n            \"display_name\": \"Page Options\",\n            \"info\": \"The page options to send with the request.\",\n        },\n        \"idempotency_key\": {\n            \"display_name\": \"Idempotency Key\",\n            \"field_type\": \"str\",\n            \"info\": \"Optional idempotency key to ensure unique requests.\",\n        },\n    }\n\n    def build(\n        self,\n        api_key: str,\n        url: str,\n        timeout: int = 30000,\n        crawlerOptions: Optional[Data] = None,\n        pageOptions: Optional[Data] = None,\n        idempotency_key: Optional[str] = None,\n    ) -> Data:\n        try:\n            from firecrawl.firecrawl import FirecrawlApp  # type: ignore\n        except ImportError:\n            raise ImportError(\n                \"Could not import firecrawl integration package. \" \"Please install it with `pip install firecrawl-py`.\"\n            )\n        if crawlerOptions:\n            crawler_options_dict = crawlerOptions.__dict__[\"data\"][\"text\"]\n        else:\n            crawler_options_dict = {}\n\n        if pageOptions:\n            page_options_dict = pageOptions.__dict__[\"data\"][\"text\"]\n        else:\n            page_options_dict = {}\n\n        if not idempotency_key:\n            idempotency_key = str(uuid.uuid4())\n\n        app = FirecrawlApp(api_key=api_key)\n        crawl_result = app.crawl_url(\n            url,\n            {\n                \"crawlerOptions\": crawler_options_dict,\n                \"pageOptions\": page_options_dict,\n            },\n            True,\n            int(timeout / 1000),\n            idempotency_key,\n        )\n\n        records = Data(data={\"results\": crawl_result})\n        return records\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "idempotency_key": {"type": "str", "required": false, "placeholder": "", "list": false, "show": true, "multiline": false, "fileTypes": [], "file_path": "", "password": false, "name": "idempotency_key", "display_name": "Idempotency Key", "advanced": false, "dynamic": false, "info": "Optional idempotency key to ensure unique requests.", "load_from_db": false, "title_case": false, "input_types": ["Text"]}, "timeout": {"type": "int", "required": false, "placeholder": "", "list": false, "show": true, "multiline": false, "value": 30000, "fileTypes": [], "file_path": "", "password": false, "name": "timeout", "display_name": "Timeout", "advanced": false, "dynamic": false, "info": "The timeout in milliseconds.", "load_from_db": false, "title_case": false}, "url": {"type": "str", "required": true, "placeholder": "", "list": false, "show": true, "multiline": false, "fileTypes": [], "file_path": "", "password": false, "name": "url", "display_name": "URL", "advanced": false, "dynamic": false, "info": "The base URL to start crawling from.", "load_from_db": false, "title_case": false, "input_types": ["Text"]}}, "description": "Firecrawl Crawl API.", "base_classes": ["Data"], "display_name": "FirecrawlCrawlApi", "documentation": "https://docs.firecrawl.dev/api-reference/endpoint/crawl", "custom_fields": {"api_key": null, "url": null, "timeout": null, "crawlerOptions": null, "pageOptions": null, "idempotency_key": null}, "output_types": ["Data"], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Data"], "selected": "Data", "name": "data", "hidden": true, "display_name": "Data", "method": null, "value": "__UNDEFINED__", "cache": true}], "field_order": ["api_key", "url", "timeout", "crawlerOptions", "pageOptions", "idempotency_key"], "beta": false, "edited": false}, "id": "FirecrawlCrawlApi-1v3BC"}, "selected": false, "width": 384, "height": 648, "positionAbsolute": {"x": 1758.1735660369038, "y": -748.8699227328705}, "dragging": false}], "edges": [{"source": "TextInput-6XxgF", "sourceHandle": "{\u0153dataType\u0153:\u0153TextInput\u0153,\u0153id\u0153:\u0153TextInput-6XxgF\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "ChatInput-NYZeX", "targetHandle": "{\u0153fieldName\u0153:\u0153sender_name\u0153,\u0153id\u0153:\u0153ChatInput-NYZeX\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "sender_name", "id": "ChatInput-NYZeX", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "TextInput", "id": "TextInput-6XxgF", "name": "text", "output_types": ["Message"]}}, "id": "reactflow__edge-TextInput-6XxgF{\u0153dataType\u0153:\u0153TextInput\u0153,\u0153id\u0153:\u0153TextInput-6XxgF\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-ChatInput-NYZeX{\u0153fieldName\u0153:\u0153sender_name\u0153,\u0153id\u0153:\u0153ChatInput-NYZeX\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "className": ""}, {"source": "TextInput-6XxgF", "sourceHandle": "{\u0153dataType\u0153:\u0153TextInput\u0153,\u0153id\u0153:\u0153TextInput-6XxgF\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "Memory-RrURK", "targetHandle": "{\u0153fieldName\u0153:\u0153session_id\u0153,\u0153id\u0153:\u0153Memory-RrURK\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "session_id", "id": "Memory-RrURK", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "TextInput", "id": "TextInput-6XxgF", "name": "text", "output_types": ["Message"]}}, "id": "reactflow__edge-TextInput-6XxgF{\u0153dataType\u0153:\u0153TextInput\u0153,\u0153id\u0153:\u0153TextInput-6XxgF\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-Memory-RrURK{\u0153fieldName\u0153:\u0153session_id\u0153,\u0153id\u0153:\u0153Memory-RrURK\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "className": ""}, {"source": "Prompt-4QaZX", "sourceHandle": "{\u0153dataType\u0153:\u0153Prompt\u0153,\u0153id\u0153:\u0153Prompt-4QaZX\u0153,\u0153name\u0153:\u0153prompt\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "OllamaModel-afIXt", "targetHandle": "{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153OllamaModel-afIXt\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "input_value", "id": "OllamaModel-afIXt", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "Prompt", "id": "Prompt-4QaZX", "name": "prompt", "output_types": ["Message"]}}, "id": "reactflow__edge-Prompt-4QaZX{\u0153dataType\u0153:\u0153Prompt\u0153,\u0153id\u0153:\u0153Prompt-4QaZX\u0153,\u0153name\u0153:\u0153prompt\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-OllamaModel-afIXt{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153OllamaModel-afIXt\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "className": ""}, {"source": "OllamaModel-afIXt", "sourceHandle": "{\u0153dataType\u0153:\u0153OllamaModel\u0153,\u0153id\u0153:\u0153OllamaModel-afIXt\u0153,\u0153name\u0153:\u0153text_output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "ChatOutput-209Yh", "targetHandle": "{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153ChatOutput-209Yh\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "input_value", "id": "ChatOutput-209Yh", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "OllamaModel", "id": "OllamaModel-afIXt", "name": "text_output", "output_types": ["Message"]}}, "id": "reactflow__edge-OllamaModel-afIXt{\u0153dataType\u0153:\u0153OllamaModel\u0153,\u0153id\u0153:\u0153OllamaModel-afIXt\u0153,\u0153name\u0153:\u0153text_output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-ChatOutput-209Yh{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153ChatOutput-209Yh\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "className": ""}, {"source": "OllamaEmbeddings-1EssT", "sourceHandle": "{\u0153dataType\u0153:\u0153OllamaEmbeddings\u0153,\u0153id\u0153:\u0153OllamaEmbeddings-1EssT\u0153,\u0153name\u0153:\u0153embeddings\u0153,\u0153output_types\u0153:[\u0153Embeddings\u0153]}", "target": "AstraDB-vQlPh", "targetHandle": "{\u0153fieldName\u0153:\u0153embedding\u0153,\u0153id\u0153:\u0153AstraDB-vQlPh\u0153,\u0153inputTypes\u0153:[\u0153Embeddings\u0153,\u0153dict\u0153],\u0153type\u0153:\u0153other\u0153}", "data": {"targetHandle": {"fieldName": "embedding", "id": "AstraDB-vQlPh", "inputTypes": ["Embeddings", "dict"], "type": "other"}, "sourceHandle": {"dataType": "OllamaEmbeddings", "id": "OllamaEmbeddings-1EssT", "name": "embeddings", "output_types": ["Embeddings"]}}, "id": "reactflow__edge-OllamaEmbeddings-1EssT{\u0153dataType\u0153:\u0153OllamaEmbeddings\u0153,\u0153id\u0153:\u0153OllamaEmbeddings-1EssT\u0153,\u0153name\u0153:\u0153embeddings\u0153,\u0153output_types\u0153:[\u0153Embeddings\u0153]}-AstraDB-vQlPh{\u0153fieldName\u0153:\u0153embedding\u0153,\u0153id\u0153:\u0153AstraDB-vQlPh\u0153,\u0153inputTypes\u0153:[\u0153Embeddings\u0153,\u0153dict\u0153],\u0153type\u0153:\u0153other\u0153}", "className": ""}, {"source": "SplitText-84vRX", "sourceHandle": "{\u0153dataType\u0153:\u0153SplitText\u0153,\u0153id\u0153:\u0153SplitText-84vRX\u0153,\u0153name\u0153:\u0153chunks\u0153,\u0153output_types\u0153:[\u0153Data\u0153]}", "target": "AstraDB-vQlPh", "targetHandle": "{\u0153fieldName\u0153:\u0153ingest_data\u0153,\u0153id\u0153:\u0153AstraDB-vQlPh\u0153,\u0153inputTypes\u0153:[\u0153Data\u0153],\u0153type\u0153:\u0153other\u0153}", "data": {"targetHandle": {"fieldName": "ingest_data", "id": "AstraDB-vQlPh", "inputTypes": ["Data"], "type": "other"}, "sourceHandle": {"dataType": "SplitText", "id": "SplitText-84vRX", "name": "chunks", "output_types": ["Data"]}}, "id": "reactflow__edge-SplitText-84vRX{\u0153dataType\u0153:\u0153SplitText\u0153,\u0153id\u0153:\u0153SplitText-84vRX\u0153,\u0153name\u0153:\u0153chunks\u0153,\u0153output_types\u0153:[\u0153Data\u0153]}-AstraDB-vQlPh{\u0153fieldName\u0153:\u0153ingest_data\u0153,\u0153id\u0153:\u0153AstraDB-vQlPh\u0153,\u0153inputTypes\u0153:[\u0153Data\u0153],\u0153type\u0153:\u0153other\u0153}", "className": ""}, {"source": "ChatInput-NYZeX", "sourceHandle": "{\u0153dataType\u0153:\u0153ChatInput\u0153,\u0153id\u0153:\u0153ChatInput-NYZeX\u0153,\u0153name\u0153:\u0153message\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "AstraDB-vQlPh", "targetHandle": "{\u0153fieldName\u0153:\u0153search_input\u0153,\u0153id\u0153:\u0153AstraDB-vQlPh\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "search_input", "id": "AstraDB-vQlPh", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "ChatInput", "id": "ChatInput-NYZeX", "name": "message", "output_types": ["Message"]}}, "id": "reactflow__edge-ChatInput-NYZeX{\u0153dataType\u0153:\u0153ChatInput\u0153,\u0153id\u0153:\u0153ChatInput-NYZeX\u0153,\u0153name\u0153:\u0153message\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-AstraDB-vQlPh{\u0153fieldName\u0153:\u0153search_input\u0153,\u0153id\u0153:\u0153AstraDB-vQlPh\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "className": ""}, {"source": "AstraDB-vQlPh", "sourceHandle": "{\u0153dataType\u0153:\u0153AstraDB\u0153,\u0153id\u0153:\u0153AstraDB-vQlPh\u0153,\u0153name\u0153:\u0153search_results\u0153,\u0153output_types\u0153:[\u0153Data\u0153]}", "target": "ParseData-uD7V0", "targetHandle": "{\u0153fieldName\u0153:\u0153data\u0153,\u0153id\u0153:\u0153ParseData-uD7V0\u0153,\u0153inputTypes\u0153:[\u0153Data\u0153],\u0153type\u0153:\u0153other\u0153}", "data": {"targetHandle": {"fieldName": "data", "id": "ParseData-uD7V0", "inputTypes": ["Data"], "type": "other"}, "sourceHandle": {"dataType": "AstraDB", "id": "AstraDB-vQlPh", "name": "search_results", "output_types": ["Data"]}}, "id": "reactflow__edge-AstraDB-vQlPh{\u0153dataType\u0153:\u0153AstraDB\u0153,\u0153id\u0153:\u0153AstraDB-vQlPh\u0153,\u0153name\u0153:\u0153search_results\u0153,\u0153output_types\u0153:[\u0153Data\u0153]}-ParseData-uD7V0{\u0153fieldName\u0153:\u0153data\u0153,\u0153id\u0153:\u0153ParseData-uD7V0\u0153,\u0153inputTypes\u0153:[\u0153Data\u0153],\u0153type\u0153:\u0153other\u0153}", "className": ""}, {"source": "Memory-RrURK", "sourceHandle": "{\u0153dataType\u0153:\u0153Memory\u0153,\u0153id\u0153:\u0153Memory-RrURK\u0153,\u0153name\u0153:\u0153messages_text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "Prompt-4QaZX", "targetHandle": "{\u0153fieldName\u0153:\u0153history\u0153,\u0153id\u0153:\u0153Prompt-4QaZX\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153,\u0153Text\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "history", "id": "Prompt-4QaZX", "inputTypes": ["Message", "Text"], "type": "str"}, "sourceHandle": {"dataType": "Memory", "id": "Memory-RrURK", "name": "messages_text", "output_types": ["Message"]}}, "id": "reactflow__edge-Memory-RrURK{\u0153dataType\u0153:\u0153Memory\u0153,\u0153id\u0153:\u0153Memory-RrURK\u0153,\u0153name\u0153:\u0153messages_text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-Prompt-4QaZX{\u0153fieldName\u0153:\u0153history\u0153,\u0153id\u0153:\u0153Prompt-4QaZX\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153,\u0153Text\u0153],\u0153type\u0153:\u0153str\u0153}"}, {"source": "ParseData-uD7V0", "sourceHandle": "{\u0153dataType\u0153:\u0153ParseData\u0153,\u0153id\u0153:\u0153ParseData-uD7V0\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "Prompt-4QaZX", "targetHandle": "{\u0153fieldName\u0153:\u0153context\u0153,\u0153id\u0153:\u0153Prompt-4QaZX\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153,\u0153Text\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "context", "id": "Prompt-4QaZX", "inputTypes": ["Message", "Text"], "type": "str"}, "sourceHandle": {"dataType": "ParseData", "id": "ParseData-uD7V0", "name": "text", "output_types": ["Message"]}}, "id": "reactflow__edge-ParseData-uD7V0{\u0153dataType\u0153:\u0153ParseData\u0153,\u0153id\u0153:\u0153ParseData-uD7V0\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-Prompt-4QaZX{\u0153fieldName\u0153:\u0153context\u0153,\u0153id\u0153:\u0153Prompt-4QaZX\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153,\u0153Text\u0153],\u0153type\u0153:\u0153str\u0153}"}, {"source": "TextInput-wm6Jy", "sourceHandle": "{\u0153dataType\u0153:\u0153TextInput\u0153,\u0153id\u0153:\u0153TextInput-wm6Jy\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "Prompt-4QaZX", "targetHandle": "{\u0153fieldName\u0153:\u0153city\u0153,\u0153id\u0153:\u0153Prompt-4QaZX\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153,\u0153Text\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "city", "id": "Prompt-4QaZX", "inputTypes": ["Message", "Text"], "type": "str"}, "sourceHandle": {"dataType": "TextInput", "id": "TextInput-wm6Jy", "name": "text", "output_types": ["Message"]}}, "id": "reactflow__edge-TextInput-wm6Jy{\u0153dataType\u0153:\u0153TextInput\u0153,\u0153id\u0153:\u0153TextInput-wm6Jy\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-Prompt-4QaZX{\u0153fieldName\u0153:\u0153city\u0153,\u0153id\u0153:\u0153Prompt-4QaZX\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153,\u0153Text\u0153],\u0153type\u0153:\u0153str\u0153}"}, {"source": "TextInput-wm6Jy", "sourceHandle": "{\u0153dataType\u0153:\u0153TextInput\u0153,\u0153id\u0153:\u0153TextInput-wm6Jy\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "WikipediaAPI-ozKXd", "targetHandle": "{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153WikipediaAPI-ozKXd\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "input_value", "id": "WikipediaAPI-ozKXd", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "TextInput", "id": "TextInput-wm6Jy", "name": "text", "output_types": ["Message"]}}, "id": "reactflow__edge-TextInput-wm6Jy{\u0153dataType\u0153:\u0153TextInput\u0153,\u0153id\u0153:\u0153TextInput-wm6Jy\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-WikipediaAPI-ozKXd{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153WikipediaAPI-ozKXd\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}"}, {"source": "WikipediaAPI-ozKXd", "sourceHandle": "{\u0153dataType\u0153:\u0153WikipediaAPI\u0153,\u0153id\u0153:\u0153WikipediaAPI-ozKXd\u0153,\u0153name\u0153:\u0153api_run_model\u0153,\u0153output_types\u0153:[\u0153Data\u0153]}", "target": "SplitText-84vRX", "targetHandle": "{\u0153fieldName\u0153:\u0153data_inputs\u0153,\u0153id\u0153:\u0153SplitText-84vRX\u0153,\u0153inputTypes\u0153:[\u0153Data\u0153],\u0153type\u0153:\u0153other\u0153}", "data": {"targetHandle": {"fieldName": "data_inputs", "id": "SplitText-84vRX", "inputTypes": ["Data"], "type": "other"}, "sourceHandle": {"dataType": "WikipediaAPI", "id": "WikipediaAPI-ozKXd", "name": "api_run_model", "output_types": ["Data"]}}, "id": "reactflow__edge-WikipediaAPI-ozKXd{\u0153dataType\u0153:\u0153WikipediaAPI\u0153,\u0153id\u0153:\u0153WikipediaAPI-ozKXd\u0153,\u0153name\u0153:\u0153api_run_model\u0153,\u0153output_types\u0153:[\u0153Data\u0153]}-SplitText-84vRX{\u0153fieldName\u0153:\u0153data_inputs\u0153,\u0153id\u0153:\u0153SplitText-84vRX\u0153,\u0153inputTypes\u0153:[\u0153Data\u0153],\u0153type\u0153:\u0153other\u0153}"}], "viewport": {"x": -1463.0850609105255, "y": 871.5206069192825, "zoom": 0.9857669942345483}}, "folder_id": "c0d5d824-0f14-47e6-a247-0c6522e738f5"}